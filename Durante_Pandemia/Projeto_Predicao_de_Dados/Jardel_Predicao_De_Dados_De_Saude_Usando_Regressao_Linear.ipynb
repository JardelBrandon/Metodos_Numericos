{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p>Disciplina:** Métodos Numéricos</p>\n",
    "**<p>Semestre:** 2020.2</p>\n",
    "**<p>Aluno:** Jardel Brandon de Araujo Regis</p>\n",
    "**<p>Mátricula:** 201621250014</p>\n",
    "**<p>2ª Unidade:** Álgebra Linear Computacional</p>\n",
    "\n",
    "---\n",
    "# <center>Projeto - 2</center>\n",
    "## <center>Predição de dados de saúde usando regressão linear</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão linear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise de regressão estuda a relação entre uma variável chamada a variável dependente e outras variáveis chamadas variáveis independentes.\n",
    "\n",
    "A relação entre elas é representada por um modelo matemático, que associa a variável dependente com as variáveis independentes.\n",
    "\n",
    "Este modelo é designado por modelo de regressão linear simples (MRLS) se define uma relação linear entre a variável dependente e uma variável independente.\n",
    "\n",
    "Se em vez de uma, forem incorporadas várias variáveis independentes, o modelo passa a denominar-se modelo de regressão linear múltipla (MRLM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ESTIMAÇÃO DOS PARÂMETROS DO MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos n observações $(n > p)$ da variável resposta e das p variáveis explicativas. Assim, yi é o valor da variável resposta na i-ésima observação enquanto que xij é o valor da variável xj na i-ésima observação, j=1,…,p. Os dados de um MRLM podem ser representados da seguinte forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\begin{array}{|l|l|l|l|l|}\n",
    "\\hline y & x_{1} & x_{2} & \\ldots & x_{p} \\\\\n",
    "\\hline y_{1} & x_{11} & x_{12} & \\ldots & x_{1 p} \\\\\n",
    "\\hline y_{2} & x_{21} & x_{22} & \\ldots & x_{2 p} \\\\\n",
    "\\hline \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\hline y_{n} & x_{n 1} & x_{n 2} & \\ldots & x_{n p} \\\\\n",
    "\\hline\n",
    "\\end{array}\\\\\n",
    "&\\text { Tabela 2.2.1: Representação dos dados }\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "em que cada observação satisfaz \n",
    "\n",
    "\\begin{equation}\n",
    "Y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\ldots+\\beta_{p} x_{i p}+\\varepsilon_{i}, \\quad i=1, \\ldots, n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método dos Mínimos Quadrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo é minimizar a função, para isso deve-se encontrar os valores de α e β que minimizam a soma dos quadrados dos\n",
    "erros (ou desvios ou resíduos), dados por\n",
    "\n",
    "\\begin{equation}\n",
    "L=\\sum_{i=1}^{n} \\varepsilon_{i}^{2}=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\beta_{1} x_{i 1}-\\beta_{2} x_{i 2}-\\ldots-\\beta_{p} x_{i p}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Obtemos então, a quantidade de informação perdida pelo modelo ou soma\n",
    "dos quadrados dos resíduos\n",
    " \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial L}{\\partial \\beta_{0}}=-2 \\sum_{i=1}^{n}\\left[Y_{i}-\\beta_{0}-\\beta_{1} x_{i 1}-\\beta_{2} x_{i 2}-\\cdots-\\beta_{p} x_{i p}\\right] \\\\\n",
    "\\frac{\\partial L}{\\partial \\beta_{j}}=-2 \\sum_{i=1}^{n}\\left[Y_{i}-\\beta_{0}-\\beta_{1} x_{i 1}-\\beta_{2} x_{i 2}-\\cdots-\\beta_{p} x_{i p}\\right] x_{j i}, \\quad j=1,2, \\ldots, p\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Igualando as derivadas parciais a zero e substituindo $ \\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p} $ por $ \\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}, \\ldots, \\widehat{\\beta}_{p}, $  temos o sistema de equaçốes \n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\{\\begin{array}{c}\n",
    "n \\widehat{\\beta}_{0}+\\widehat{\\beta}_{1} \\sum_{i=1}^{n} x_{i 1}+\\widehat{\\beta}_{2} \\sum_{i=1}^{n} x_{i 2}+\\ldots+\\widehat{\\beta}_{p} \\sum_{i=1}^{n} x_{i p}=\\sum_{i=1}^{n} Y_{i} \\\\\n",
    "\\widehat{\\beta}_{0} \\sum_{i=1}^{n} x_{i 1}+\\widehat{\\beta}_{1} \\sum_{i=1}^{n} x_{i 1}^{2}+\\widehat{\\beta}_{2} \\sum_{i=1}^{n} x_{i 1} x_{i 2}+\\ldots+\\widehat{\\beta}_{p} \\sum_{i=1}^{n} x_{i 1} x_{i p}=\\sum_{i=1}^{n} x_{i 1} Y_{i} \\\\\n",
    "\\vdots \\\\\n",
    "\\widehat{\\beta}_{0} \\sum_{i=1}^{n} x_{i p}+\\widehat{\\beta}_{1} \\sum_{i=1}^{n} x_{i p} x_{i 1}+\\widehat{\\beta}_{2} \\sum_{i=1}^{n} x_{i p} x_{i 2}+\\ldots+\\widehat{\\beta}_{p} \\sum_{i=1}^{n} \\frac{2}{i p}=\\sum_{i=1}^{n} x_{i p} Y_{i}\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Resolvendo este sistema, obtemos os estimadores de mínimos quadrados  $ \\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}, \\ldots, \\widehat{\\beta}_{p}, $ dos parâmetros do modelo em questão. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representação matricial do MRLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pode-se notar que os estimadores de mínimos quadrados dos parâmetros podem ser facilmente encontrados considerando a notação matricial dos dados, que é de fácil manipulação. Desta forma, o modelo de Regressão Linear Múltipla pode ser escrito como  $$ Y=Xβ+ε, $$\n",
    "Com\n",
    "\n",
    "\\begin{equation}\n",
    "Y=\\left[\\begin{array}{c}\n",
    "Y_{1} \\\\\n",
    "Y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "Y_{n}\n",
    "\\end{array}\\right], \\quad X=\\left[\\begin{array}{cccc}\n",
    "1 & x_{11} & x_{12} & \\ldots & x_{1 p} \\\\\n",
    "1 & x_{21} & x_{22} & \\ldots & x_{2 p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n 1} & x_{n 2} & \\ldots & x_{n p}\n",
    "\\end{array}\\right] \\quad, \\quad \\beta=\\left[\\begin{array}{c}\n",
    "\\beta_{0} \\\\\n",
    "\\beta_{1} \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_{p}\n",
    "\\end{array}\\right] \\quad \\text { e } \\varepsilon=\\left[\\begin{array}{c}\n",
    "\\varepsilon_{1} \\\\\n",
    "\\varepsilon_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_{n}\n",
    "\\end{array}\\right],\n",
    "\\end{equation}\n",
    "\n",
    "em que\n",
    "\n",
    "- Y é um vetor n×1 cujos componentes corresponde às n respostas;\n",
    "- X é uma matriz de dimensão n×(p+1) denominada matriz do modelo;\n",
    "- ε é um vetor de dimensão n×1 cujos componentes são os erros e\n",
    "- β é um vetor (p+1)×1 cujos elementos são os coeficientes de regressão. \n",
    "\n",
    "O método de mínimos quadrados tem como objetivo encontrar o vetor $ \\widehat{\\beta} $ que minimiza\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "L=\\sum_{i=1}^{n} \\varepsilon_{i}^{2}=\\varepsilon^{\\prime} \\varepsilon=(Y-X \\beta)^{\\prime}(Y-X \\beta)= \\\\\n",
    "=Y^{\\prime} Y-Y^{\\prime} X \\beta-\\beta^{\\prime} X^{\\prime} Y+\\beta^{\\prime} X^{\\prime} X \\beta=Y^{\\prime} Y-2 \\beta^{\\prime} X^{\\prime} Y+\\beta^{\\prime} X^{\\prime} X \\beta\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "sendo que $ Y′Xβ=β′X′Y $ pois o produto resulta em um escalar. A notação $ X′ $ representa o transposto da matriz X enquanto que $ Y′ $ e $ β′ $ representam os transpostos dos vetores $ Y $ e $ β $, respectivamente. Usando a técnica de derivação (em termos matriciais) obtemos\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\beta}=-2 X^{\\prime} Y+2 X^{\\prime} X \\beta\n",
    "\\end{equation}\n",
    "\n",
    "Igualando a zero e substituindo o vetor β pelo vetor $ \\widehat{\\beta} $, temos\n",
    "\n",
    "$$ (X′X)\\widehat{\\beta} = X′Y $$\n",
    "\n",
    "Em geral, a matriz $ (X′X) $ é não singular, ou seja, tem determinante diferente de zero, e portanto é invertível. Desta forma, conclui-se que os estimadores para os parâmetros $ βj,\\ j=0, \\dots , p $ são dados pelo vetor\n",
    "\n",
    "$$ \\widehat{\\beta}=(X′X)^{−1}X′Y $$\n",
    "\n",
    "Portanto, o modelo de regressão linear ajustado e o vetor de resíduos são respectivamente\n",
    "\n",
    "$$ \\widehat{Y} = X\\widehat{\\beta} \\quad e \\quad e=Y−Yˆ=Y−Yˆ$$\n",
    "\n",
    "Assim, ao substituir os estimadores de mínimos quadrados, obtêm-se que $ \\widehat{Y} = HY $ no qual $ H = X(X′X)^{−1}X′ $ é a matriz chapéu, ou matriz de projeção do vetor de respostas $ Y $ no vetor de respostas ajustadas $ \\widehat{Y} $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math, scipy, numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de dados retirados de exames de diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os conjuntos de dados foram retirados de pacientes com diabetes. Os dados consistem em 442 amostras e 10 variáveis (todas são características fisiológicas), por isso todo o grupo é composto de pessoas altas e magras. A variável dependente é uma medida quantitativa da progressão da doença um ano após o início do estudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um conjunto de dados clássico, conhecido por Efron, Hastie, Johnstone e Tibshirani em seu artigo [Least Angle Regression](https://arxiv.org/pdf/math/0406456.pdf) e um dos [muitos conjuntos de dados incluído no scikit-learn](http://scikit-learn.org/stable/datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn,test,y_trn,y_test = train_test_split(data.data, data.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((353, 10), (89, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão linear no Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere um sistema $X\\beta = y$, onde $X$ tem mais linhas do que colunas. Isso ocorre quando você tem mais amostras de dados do que variáveis. Queremos encontrar $\\hat{\\beta}$ que minimize: \n",
    "$$ \\big\\vert\\big\\vert X\\beta - y \\big\\vert\\big\\vert_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using the sklearn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773 µs ± 171 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "%timeit regr.fit(trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = regr.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be helpful to have some metrics on how good our prediciton is.  We will look at the mean squared norm (L2) and mean absolute error (L1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regr_metrics(act, pred):\n",
    "    return (math.sqrt(metrics.mean_squared_error(act, pred)), \n",
    "     metrics.mean_absolute_error(act, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.40683110341174, 44.6858312920976)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_metrics(y_test, regr.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos polinomiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressão linear encontra os melhores coeficientes $\\beta_i$ para:\n",
    "\n",
    "$$ x_0\\beta_0 + x_1\\beta_1 + x_2\\beta_2 = y $$\n",
    "\n",
    "Adicionando os termos polinomiais, ainda se trata de um problema de regressão linear, apenas com mais termos:\n",
    "\n",
    "$$ x_0\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_0^2\\beta_3 + x_0 x_1\\beta_4 + x_0 x_2\\beta_5 + x_1^2\\beta_6 + x_1 x_2\\beta_7 + x_2^2\\beta_8 = y $$\n",
    "\n",
    "Utilizando os dados originais $X$ para calcular as características polinomiais adicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para tentar melhorar o desempenho do modelo foi adicionado mais alguns recursos. Atualmente, o modelo é linear em cada variável, mas pode-se adicionar recursos polinomiais para alterar isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_feat = poly.fit_transform(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'age, sex, bmi, bp, s1, s2, s3, s4, s5, s6, age^2, age sex, age bmi, age bp, age s1, age s2, age s3, age s4, age s5, age s6, sex^2, sex bmi, sex bp, sex s1, sex s2, sex s3, sex s4, sex s5, sex s6, bmi^2, bmi bp, bmi s1, bmi s2, bmi s3, bmi s4, bmi s5, bmi s6, bp^2, bp s1, bp s2, bp s3, bp s4, bp s5, bp s6, s1^2, s1 s2, s1 s3, s1 s4, s1 s5, s1 s6, s2^2, s2 s3, s2 s4, s2 s5, s2 s6, s3^2, s3 s4, s3 s5, s3 s6, s4^2, s4 s5, s4 s6, s5^2, s5 s6, s6^2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(poly.get_feature_names(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 65)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(trn_feat, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.11138664286574, 52.13794990801871)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_metrics(y_test, regr.predict(poly.fit_transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tempo é elevado ao quadrado em #features e linear em #points, dessa forma, a tendência é de lentidão computacional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560 µs ± 36.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit poly.fit_transform(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acelerando a geração dos recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acelerar isso. pode-se utilizar a [Numba](http://numba.pydata.org/numba-doc/0.12.2/tutorial_firststeps.html), uma biblioteca Python que compila o código diretamente para C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Este tutorial] (https://jakevdp.github.io/blog/2012/08/24/numba-vs-cython/) de Jake VanderPlas é uma boa introdução. Aqui Jake [implementa um algoritmo não trivial](https://jakevdp.github.io/blog/2015/02/24/optimizing-python-with-numpy-and-numba/) (transformação rápida de Fourier não uniforme) com Numba. Cython é outra alternativa. Descobri que o Cython exige mais conhecimento para ser usado do que o Numba (é mais próximo do C), mas fornece acelerações semelhantes ao Numba.\n",
    "\n",
    "Aqui está uma [resposta completa](https://softwareengineering.stackexchange.com/questions/246094/understanding-the-differences-traditional-interpreter-jit-compiler-jit-interp) sobre as diferenças entre um Ahead Of Time (AOT ), um compilador Just In Time (JIT) e um interpretador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos com vetorização e com código nativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, para se familiarizar com o Numba e, em seguida, retornamos ao problema de características polinomiais para regressão no conjunto de dados de diabates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy as np, matplotlib.pyplot as plt\n",
    "from pandas_summary import DataFrameSummary\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, vectorize, guvectorize, cuda, float32, void, float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será possível observar o impacto de: \n",
    "- Evitar alocações de memória e cópias (mais lento do que cálculos de CPU)\n",
    "- Melhor localizações \n",
    "- Vetorização \n",
    "\n",
    "Se for utilizado o numpy em arrays inteiros de uma vez, isso criará muitos temporários e será possível usar o cache. Se for utilizado o loop numba por meio de um item do array por vez, não precisamos alocar grandes arrays temporários e pode-se reutilizar os dados armazenados em cache, pois está sendo realizado vários cálculos em cada item do array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untype and Unvectorized\n",
    "def proc_python(xx,yy):\n",
    "    zz = np.zeros(nobs, dtype='float32')\n",
    "    for j in range(nobs):   \n",
    "        x, y = xx[j], yy[j] \n",
    "        x = x*2 - ( y * 55 )\n",
    "        y = x + y*2         \n",
    "        z = x + y + 99      \n",
    "        z = z * ( z - .88 ) \n",
    "        zz[j] = z           \n",
    "    return zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 10000\n",
    "x = np.random.randn(nobs).astype('float32')\n",
    "y = np.random.randn(nobs).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ms ± 23.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit proc_python(x,y)   # Untyped and unvectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando Numpy é possível realizar a vetorizarização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_numpy(x,y):\n",
    "    z = np.zeros(nobs, dtype='float32')\n",
    "    x = x*2 - ( y * 55 )\n",
    "    y = x + y*2         \n",
    "    z = x + y + 99      \n",
    "    z = z * ( z - .88 ) \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose( proc_numpy(x,y), proc_python(x,y), atol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.3 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit proc_numpy(x,y)    # Typed and vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba oferece vários #decorators diferentes. Tentaremos dois diferentes: \n",
    "- `@ jit`: muito geral \n",
    "- `@ vectorize`: não precisa escrever um loop for. útil ao operar em vetores do mesmo tamanho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, será utilizado o decorator de compilador `jit` (just-in-time) do Numba, sem vetorizar explicitamente. Isso evita grandes alocações de memória, então temos melhor locação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@jit()\n",
    "def proc_numba(xx,yy,zz):\n",
    "    for j in range(nobs):   \n",
    "        x, y = xx[j], yy[j] \n",
    "        x = x*2 - ( y * 55 )\n",
    "        y = x + y*2         \n",
    "        z = x + y + 99      \n",
    "        z = z * ( z - .88 ) \n",
    "        zz[j] = z           \n",
    "    return zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.zeros(nobs).astype('float32')\n",
    "np.allclose( proc_numpy(x,y), proc_numba(x,y,z), atol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 µs ± 1.34 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit proc_numba(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, será utilizado o decorator `vectorize` do Numba. O compilador do Numba otimiza isso de uma maneira mais inteligente do que é possível com Python e Numpy simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize\n",
    "def vec_numba(x,y):\n",
    "    x = x*2 - ( y * 55 )\n",
    "    y = x + y*2         \n",
    "    z = x + y + 99      \n",
    "    return z * ( z - .88 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(vec_numba(x,y), proc_numba(x,y,z), atol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5 µs ± 4.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vec_numba(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, observa-se o poder do **Numba** que é impressionante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Recursos polinomiais do Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def vec_poly(x, res):\n",
    "    m,n=x.shape\n",
    "    feat_idx=0\n",
    "    for i in range(n):\n",
    "        v1=x[:,i]\n",
    "        for k in range(m): res[k,feat_idx] = v1[k]\n",
    "        feat_idx+=1\n",
    "        for j in range(i,n):\n",
    "            for k in range(m): res[k,feat_idx] = v1[k]*x[k,j]\n",
    "            feat_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Row-Major vs Column-Major Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Desta [postagem no blog de Eli Bendersky] (http://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays/): \"O layout da linha principal de uma matriz coloca a primeira linha na memória contígua, depois a segunda linha logo após, a terceira e assim por diante. O layout da coluna principal coloca a primeira coluna na memória contígua, depois a segunda, etc. .... Embora saber qual layout um determinado conjunto de dados está usando seja crítico para um bom desempenho, não há uma resposta única para a questão de qual layout 'é melhor' em geral. \n",
    "\n",
    "\"Acontece que combinar a maneira como seu algoritmo funciona com o layout de dados pode melhorar ou prejudicar o desempenho de um aplicativo. \n",
    "\n",
    "\"O resumo é: **sempre cruze os dados na ordem em que foram dispostos**.\" \n",
    "\n",
    "**Layout da coluna principal**: Fortran, Matlab, R e Julia \n",
    "\n",
    "**Layout de linha principal**: C, C ++, Python, Pascal, Mathematica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trn = np.asfortranarray(trn)\n",
    "test = np.asfortranarray(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m,n=trn.shape\n",
    "n_feat = n*(n+1)//2 + n\n",
    "trn_feat = np.zeros((m,n_feat), order='F')\n",
    "test_feat = np.zeros((len(y_test), n_feat), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vec_poly(trn, trn_feat)\n",
    "vec_poly(test, test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(trn_feat, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.1113866428662, 52.137949908019266)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_metrics(y_test, regr.predict(test_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5 µs ± 1.26 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vec_poly(trn, trn_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lembrete de que essa foi a época da implementação do scikit learn PolynomialFeatures, que foi criada por especialistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 µs ± 102 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit poly.fit_transform(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como é possível observar o Numba apresenta um ganho de performace incrível! Com uma única linha de código, geralmente em média dependendo do poder computacional se é obtido uma aceleração de 78 vezes em relação ao scikit learn (que foi otimizado por especialistas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Regularização e ruído"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A regularização é uma forma de reduzir o sobreajuste e criar modelos que generalizem melhor para obtenção de novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A regressão Lasso usa uma penalidade L1, que empurra para coeficientes esparsos. O parâmetro $\\alpha$ é usado para ponderar o termo de penalidade. O LassoCV do Scikit Learn realiza validação cruzada com vários valores diferentes para $\\alpha$. Assista a este [vídeo do Coursera sobre regressão Lasso](https://www.coursera.org/learn/machine-learning-data-analysis/lecture/0KIy7/what-is-lasso-regression) para obter mais informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg_regr = linear_model.LassoCV(n_alphas=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarde\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:527: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14633.597475241055, tolerance: 167.76957021276598\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\jarde\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:527: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 37450.013853975805, tolerance: 169.5592570921986\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\jarde\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:527: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6187.253244669642, tolerance: 166.78198657243817\n",
      "  tol, rng, random, positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(n_alphas=10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_regr.fit(trn_feat, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009656131375463322"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_regr.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.22433795374341, 44.09299567937261)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_metrics(y_test, reg_regr.predict(test_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ruído"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Agora será adicionado algum ruído aos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idxs = np.random.randint(0, len(trn), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_trn2 = np.copy(y_trn)\n",
    "y_trn2[idxs] *= 10 # label noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.406831103411726, 44.685831292097596)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(trn, y_trn)\n",
    "regr_metrics(y_test, regr.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.08903254608788, 58.642846024731185)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(trn, y_trn2)\n",
    "regr_metrics(y_test, regr.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A perda de Huber é uma função de perda menos sensível a outliers do que a perda de erro quadrático. É quadrático para pequenos valores de erro e linear para grandes valores.\n",
    "\n",
    " $$L(x)= \n",
    "\\begin{cases}\n",
    "    \\frac{1}{2}x^2,         & \\text{para } \\lvert x\\rvert\\leq \\delta \\\\\n",
    "    \\delta(\\lvert x \\rvert - \\frac{1}{2}\\delta),  & \\text{Caso contrário}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jarde\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54.70857501798143, 45.301044643818095)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hregr = linear_model.HuberRegressor()\n",
    "hregr.fit(trn, y_trn2)\n",
    "regr_metrics(y_test, hregr.predict(test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
